name: Test Flash Attention Build (Small Matrix)

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Version tag of flash-attention to build: (format: v2.6.3)'
        default: 'v2.6.3'
        required: true
        type: string

permissions:
  contents: write

jobs:
  test_build:
    name: Test build for OS ${{ matrix.os }}, Python ${{ matrix.pyver }}, CUDA ${{ matrix.cuda }}, Torch ${{ matrix.torchver }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-22.04"]  # Test on Ubuntu only
        pyver: ["3.10"]       # Single Python version
        cuda: ["12.4.1"]      # Single CUDA version
        torchver: ["2.5.1"]   # Single PyTorch version
    defaults:
      run:
        shell: bash
    env:
        CUDAVER: ${{ matrix.cuda }}
        PCKGVER: ${{ inputs.version }}
        FLASH_ATTENTION_REPO: "https://github.com/Dao-AILab/flash-attention.git"

    steps:
      - name: Checkout this repository
        uses: actions/checkout@v4

      - name: Clone flash-attention repository
        run: |
          git clone --depth 1 --branch ${{ inputs.version }} ${{ env.FLASH_ATTENTION_REPO }} flash-attention-src
          cd flash-attention-src
          git submodule update --init --recursive

      - name: Install Linux CUDA ${{ matrix.cuda }}
        uses: Jimver/cuda-toolkit@v0.2.23
        id: cuda-toolkit-Linux
        with:
          cuda: "${{ matrix.cuda }}"
          linux-local-args: '["--toolkit"]'
          method: "network"
          sub-packages: '["nvcc"]'

      - name: Install the latest version of uv and set the python version
        uses: astral-sh/setup-uv@v5
        with:
          python-version: ${{ matrix.pyver }}

      - name: Install Dependencies
        run: |
          cudaVersion="${{ matrix.cuda }}"
          cudaVersionPytorch=$(echo $cudaVersion | sed 's/\.[^.]*$//' | sed 's/\.//g')
          pytorchIndexUrl="https://download.pytorch.org/whl/cu$cudaVersionPytorch"

          uv pip install --upgrade build setuptools wheel packaging ninja torch==${{ matrix.torchver }} psutil --extra-index-url $pytorchIndexUrl

      - name: Test Build Setup (Dry Run)
        working-directory: flash-attention-src
        run: |
          export CUDA_PATH=$CONDA_PREFIX
          export CUDA_HOME=$CONDA_PREFIX
          export MAX_JOBS=1
          export FLASH_ATTENTION_FORCE_BUILD=TRUE
          export FLASH_ATTENTION_FORCE_CXX11_ABI=FALSE
          export BUILD_TARGET=cuda

          echo "Testing build environment..."
          echo "CUDA Version: ${{ matrix.cuda }}"
          echo "PyTorch Version: ${{ matrix.torchver }}"
          echo "Python Version: ${{ matrix.pyver }}"
          echo "Flash Attention Version: ${{ inputs.version }}"
          
          # Test if we can import torch and check CUDA
          python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"
          
          # List setup.py to ensure it exists
          ls -la setup.py
          
          # Test setup.py without actually building
          python setup.py --help-commands

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-build-logs-${{ matrix.os }}-py${{ matrix.pyver }}-cuda${{ matrix.cuda }}-torch${{ matrix.torchver }}
          path: |
            flash-attention-src/setup.py
            flash-attention-src/requirements.txt 