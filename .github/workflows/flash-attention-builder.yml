name: Flash Attention Builder

on:
  workflow_dispatch:
    inputs:
      version:
        description: 'Flash Attention version tag'
        default: 'v2.6.3'
        required: true
        type: string
      cuda_version:
        description: 'CUDA version'
        required: true
        default: '12.1.1'
        type: choice
        options:
          - '11.8.0'
          - '12.1.1'
          - '12.2.2'
          - '12.3.2'
          - '12.4.1'

permissions:
  contents: write

jobs:
  build:
    name: Build FA ${{ inputs.version }} CUDA ${{ inputs.cuda_version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04]
        python: ["3.10", "3.11"]
        torch: ["2.4.1", "2.5.1"]
        exclude:
          # Skip problematic combinations for now
          - python: "3.12"
          - torch: "2.6.0"
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Free Disk Space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf "/usr/local/share/boost"
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"

      - name: Clone Flash Attention ${{ inputs.version }}
        run: |
          echo "Cloning flash-attention ${{ inputs.version }}..."
          git clone --depth 1 --branch ${{ inputs.version }} https://github.com/Dao-AILab/flash-attention.git fa-src
          cd fa-src
          echo "Updating submodules..."
          git submodule update --init --recursive --depth 1
          echo "âœ… Successfully cloned Flash Attention ${{ inputs.version }}"

      - name: Setup Python ${{ matrix.python }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python }}

      - name: Install CUDA ${{ inputs.cuda_version }}
        uses: Jimver/cuda-toolkit@v0.2.23
        with:
          cuda: ${{ inputs.cuda_version }}
          method: network
          sub-packages: '["nvcc", "cudart", "cublas", "cufft", "curand", "cusolver", "cusparse"]'

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake

      - name: Install PyTorch ${{ matrix.torch }}
        run: |
          cudaVersion="${{ inputs.cuda_version }}"
          cudaShort=$(echo $cudaVersion | sed 's/\.[^.]*$//' | sed 's/\.//g')
          echo "Installing PyTorch ${{ matrix.torch }} for CUDA $cudaShort"
          pip install torch==${{ matrix.torch }} --index-url https://download.pytorch.org/whl/cu$cudaShort

      - name: Install Build Dependencies
        run: |
          pip install ninja packaging wheel setuptools psutil

      - name: Verify Environment
        run: |
          echo "=== Environment Check ==="
          python -c "import torch; print(f'âœ… PyTorch: {torch.__version__}'); print(f'âœ… CUDA available: {torch.cuda.is_available()}'); print(f'âœ… CUDA device count: {torch.cuda.device_count()}')"
          nvcc --version
          echo "PATH: $PATH"
          echo "CUDA_HOME: $CUDA_HOME"

      - name: Build Flash Attention
        working-directory: fa-src
        run: |
          echo "=== Building Flash Attention ==="
          export MAX_JOBS=1
          export FLASH_ATTENTION_FORCE_BUILD=TRUE
          export FLASH_ATTENTION_FORCE_CXX11_ABI=FALSE
          export BUILD_TARGET=cuda
          export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9;9.0"
          
          # Set CUDA paths
          export CUDA_HOME=$CUDA_PATH
          export PATH=$CUDA_PATH/bin:$PATH
          export LD_LIBRARY_PATH=$CUDA_PATH/lib64:$LD_LIBRARY_PATH
          
          echo "Building with MAX_JOBS=1 to avoid memory issues..."
          echo "CUDA_HOME: $CUDA_HOME"
          echo "CUDA_PATH: $CUDA_PATH"
          
          python setup.py bdist_wheel --dist-dir=dist
          
          echo "=== Build Complete ==="
          ls -la dist/
          
          # Rename wheel with CUDA and PyTorch info
          cd dist
          for wheel in *.whl; do
            if [ -f "$wheel" ]; then
              version=$(echo "${{ inputs.version }}" | sed 's/^v//')
              cuda_short=$(echo "${{ inputs.cuda_version }}" | sed 's/\.[^.]*$//' | sed 's/\.//g')
              new_name=$(echo "$wheel" | sed "s/flash_attn-$version-/flash_attn-$version+cu${cuda_short}torch${{ matrix.torch }}-/")
              mv "$wheel" "$new_name"
              echo "âœ… Renamed: $wheel -> $new_name"
            fi
          done

      - name: Upload Wheel Artifact
        uses: actions/upload-artifact@v4
        with:
          name: flash-attn-${{ inputs.version }}-${{ matrix.os }}-py${{ matrix.python }}-cuda${{ inputs.cuda_version }}-torch${{ matrix.torch }}
          path: fa-src/dist/*.whl
          retention-days: 30

      - name: Upload to Release
        uses: svenstaro/upload-release-action@2.6.1
        continue-on-error: true
        with:
          file: fa-src/dist/*.whl
          tag: ${{ inputs.version }}-builds
          file_glob: true
          overwrite: true
          release_name: "Flash Attention ${{ inputs.version }} Wheels"
          body: |
            ## Flash Attention ${{ inputs.version }} Pre-built Wheels
            
            **Build Configuration:**
            - ğŸ·ï¸ Version: `${{ inputs.version }}`
            - ğŸ”§ CUDA: `${{ inputs.cuda_version }}`
            - ğŸ Python: `${{ matrix.python }}`
            - ğŸ”¥ PyTorch: `${{ matrix.torch }}`
            - ğŸ’» OS: `${{ matrix.os }}`
            
            **Installation:**
            ```bash
            pip install <wheel_name>
            ```
            
            **Source:** [flash-attention ${{ inputs.version }}](https://github.com/Dao-AILab/flash-attention/tree/${{ inputs.version }}) 