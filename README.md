# Flash Attention Builder

Build flash-attention wheels for any CUDA and PyTorch combination from the official [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention) repository.

## ğŸ¯ Features

- âœ… **One Click Building** - Simple dropdown inputs
- âœ… **CUDA Selection** - Choose from 11.8 to 12.8 versions  
- âœ… **Multi-Platform** - Windows & Linux builds
- âœ… **Auto Release** - Wheels uploaded to GitHub releases
- âœ… **Proper Naming** - Includes CUDA & PyTorch versions

## ğŸš€ Usage

1. Go to **[Actions](https://github.com/tuankiet2s/flash-attention-builder/actions)** tab
2. Click **"Flash Attention Builder"**
3. Click **"Run workflow"**
4. Select options:
   - **Version**: Flash attention tag (e.g., `v2.6.3`)
   - **CUDA**: Pick from dropdown (11.8.0 to 12.8.1)
5. Click **"Run workflow"**

## ğŸ“Š Build Matrix

**Per run, builds for:**
- ğŸ **Python**: 3.10, 3.11, 3.12
- ğŸ”¥ **PyTorch**: 2.4.1, 2.5.1, 2.6.0  
- ğŸ’» **OS**: Ubuntu + Windows (Windows has fewer combinations)
- **Total**: ~12-14 wheel variants per CUDA version

## ğŸ“¦ Download Wheels

Built wheels are available in [Releases](https://github.com/tuankiet2s/flash-attention-builder/releases):

```bash
# Example wheel names:
flash_attn-2.6.3+cu124torch2.5.1-cp310-cp310-linux_x86_64.whl
flash_attn-2.6.3+cu124torch2.6.0-cp311-cp311-win_amd64.whl
```

## ğŸ“‹ Installation

```bash
# Download the wheel for your setup and install:
pip install flash_attn-2.6.3+cu124torch2.5.1-cp310-cp310-linux_x86_64.whl

# Or use GitHub CLI:
gh release download v2.6.3-builds --repo tuankiet2s/flash-attention-builder
pip install flash_attn-*.whl
```

## âš¡ Quick Test

Test with GitHub CLI:
```bash
gh workflow run "Flash Attention Builder" \
  --repo tuankiet2s/flash-attention-builder \
  -f version=v2.6.3 \
  -f cuda_version=12.4.1
```

## ğŸ” Source

- **Built from**: [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)
- **Supports**: All official flash-attention releases 